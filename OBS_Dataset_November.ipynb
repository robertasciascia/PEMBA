{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import time\n",
    "from datetime import datetime\n",
    "from scipy import interpolate\n",
    "import cartopy.crs as ccrs\n",
    "import pyroms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA HALF HOUR VELOCITY FOR THE CASE OF NOVEMBER\n",
    "\n",
    "Nb the grid is the same for the April and November so it's only created in the April Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://dap.ceda.ac.uk/thredds/dodsC/bodc/UOX220077/WINDS-M/1997/WINDS-M_SFC_1997.nc'\n",
    "ds_complete_1997 = xr.open_dataset(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://dap.ceda.ac.uk/thredds/dodsC/bodc/UOX220077/WINDS-M/1998/WINDS-M_SFC_1998.nc'\n",
    "ds_complete_1998 = xr.open_dataset(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary variables\n",
    "dropping_variables = ds_complete_1997.drop_vars(['s_w','hc','s_rho',\n",
    "                                 'theta_s','theta_b','Tcline','Vtransform','h','f','pm','pn','lon_rho','lat_rho',\n",
    "                                  'angle', 'Cs_r','sc_r','Cs_w','lon_u','lon_v','lat_u', 'lat_v',\n",
    "                                  'sc_w','mask_rho'])\n",
    "\n",
    "# Clipping considering only the domain of interest for the thesis case study\n",
    "clipped_domain_1997 = dropping_variables.sel(y_rho=slice(680, 986), x_rho=slice(150, 451),\n",
    "                          y_u=slice(680, 986), x_u=slice(150, 451),\n",
    "                          y_v=slice(680, 986), x_v=slice(150, 451))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary variables\n",
    "dropping_variables = ds_complete_1998.drop_vars(['s_w','hc','s_rho',\n",
    "                                 'theta_s','theta_b','Tcline','Vtransform','h','f','pm','pn','lon_rho','lat_rho',\n",
    "                                  'angle', 'Cs_r','sc_r','Cs_w','lon_u','lon_v','lat_u', 'lat_v',\n",
    "                                  'sc_w','mask_rho'])\n",
    "\n",
    "# Clipping considering only the domain of interest for the thesis case study\n",
    "clipped_domain_1998 = dropping_variables.sel(y_rho=slice(680, 986), x_rho=slice(150, 451),\n",
    "                          y_u=slice(680, 986), x_u=slice(150, 451),\n",
    "                          y_v=slice(680, 986), x_v=slice(150, 451))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NOTE: Case of November as spawning month\n",
    "\n",
    "Since starting from 1st November 1997, the 100th day after is the 8th February 1998, the 1998 dataset must be used too, with the same process done above.\n",
    "\n",
    "To concatenate the two dataset with 61 + 39 days, it is possible to proceed in this way (it is very fast [9sec with rechunking, 15min without]):\n",
    "- Chunk sizes\n",
    "  \n",
    "chunk_sizes = {'time_counter': 1, 'y_rho': 301, 'x_rho': 301, 'y_v': 301, 'x_v': 301, 'y_u': 301, 'x_u': 301}\n",
    "- Rechunking the two datasets (the one with values of November and December and the one of January and February)\n",
    "  \n",
    "nov_dec_1997_rechunked = nov_dec_1997.chunk(chunk_sizes)\n",
    "\n",
    "jan_feb_1998_rechunked = jan_feb_1998.chunk(chunk_sizes)\n",
    "- Creating the rechunked 100 days dataset\n",
    "  \n",
    "days70_from_november_1997 = xr.concat([nov_dec_1997_rechunked, jan_feb_1998_rechunked], dim='time_counter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the time interval Nov 1st 31 Dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering only 61 from Nov 1st\n",
    "days_from_Nov_1997 = clipped_domain_1997.isel(time_counter=slice(14592,17520))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the time interval from Jan 1st to Feb 8th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering only 39 from Jan 1st\n",
    "days_from_Jan_1998 = clipped_domain_1998.isel(time_counter=slice(0,1873))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_sizes = {'time_counter': 1, 'y_rho': 306, 'x_rho': 301, 'y_v': 306, 'x_v': 301,\n",
    "               'y_u':306, 'x_u': 301}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_from_Nov_1997_rechunked = days_from_Nov_1997.chunk(chunk_sizes)\n",
    "days_from_Jan_1998_rechunked = days_from_Jan_1998.chunk(chunk_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <html^><head><title>500 Internal Server Error</title></head><body><center><h1>500 Internal Server Error</h1></center><hr><center>nginx</center></body></html>\n"
     ]
    }
   ],
   "source": [
    "days100_from_november_1997 = xr.concat([days_from_Nov_1997_rechunked, days_from_Jan_1998_rechunked], dim='time_counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "days100_from_november_1997_velocity_u = days100_from_november_1997.drop_vars(['nav_lon_v', 'nav_lat_v', 'v_surf', 'time'])\n",
    "days100_from_november_1997_velocity_v = days100_from_november_1997.drop_vars(['nav_lon_u', 'nav_lat_u', 'u_surf', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <html^><head><title>500 Internal Server Error</title></head><body><center><h1>500 Internal Server Error</h1></center><hr><center>nginx</center></body></html>\n",
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <html^><head><title>500 Internal Server Error</title></head><body><center><h1>500 Internal Server Error</h1></center><hr><center>nginx</center></body></html>\n",
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <html^><head><title>500 Internal Server Error</title></head><body><center><h1>500 Internal Server Error</h1></center><hr><center>nginx</center></body></html>\n",
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <html^><head><title>500 Internal Server Error</title></head><body><center><h1>500 Internal Server Error</h1></center><hr><center>nginx</center></body></html>\n"
     ]
    }
   ],
   "source": [
    "chunks = {'time_counter': 100, 'y_u': 306, 'x_u': 301}\n",
    "days100_from_november_1997_velocity_u_chunked = days100_from_november_1997_velocity_u.chunk(chunks)\n",
    "days100_from_november_1997_velocity_u_chunked.to_netcdf(\"/mnt/iscsi2/OceanParcels/InputObsDom/days100_from_november_1997_velocity_u_OK.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <html^><head><title>500 Internal Server Error</title></head><body><center><h1>500 Internal Server Error</h1></center><hr><center>nginx</center></body></html>\n",
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <html^><head><title>500 Internal Server Error</title></head><body><center><h1>500 Internal Server Error</h1></center><hr><center>nginx</center></body></html>\n",
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: <html^><head><title>500 Internal Server Error</title></head><body><center><h1>500 Internal Server Error</h1></center><hr><center>nginx</center></body></html>\n"
     ]
    }
   ],
   "source": [
    "chunks = {'time_counter': 100, 'y_v': 306, 'x_v': 301}\n",
    "days100_from_november_1997_velocity_v_chunked = days100_from_november_1997_velocity_v.chunk(chunks)\n",
    "days100_from_november_1997_velocity_v_chunked.to_netcdf(\"/mnt/iscsi2/OceanParcels/InputObsDom/days100_from_november_1997_velocity_v_OK.nc\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
